{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raw_source\n",
    "import math\n",
    "from tqdm import tqdm,trange\n",
    "class Dataset:\n",
    "    def __init__(self,text,labels,masks):\n",
    "        self.text = torch.tensor(text)\n",
    "        self.labels = torch.tensor(labels)\n",
    "        self.masks = torch.from_numpy(masks)\n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index],self.labels[index],self.masks[index]\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180000it [00:00, 344731.02it/s]\n",
      "10000it [00:00, 589949.36it/s]\n",
      "10000it [00:00, 526327.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train_text,train_labels,train_masks,dev_text,dev_labels,dev_masks,test_text,test_labels,test_masks,vocab_size,train_contents,dev_contents,test_contents = raw_source.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本： 两天价网站背后重重迷雾：做个网站究竟要多少钱\n",
      "ont hot编码: [135, 80, 33, 54, 505, 1032, 70, 95, 95, 681, 2288, 4, 486, 179, 54, 505, 626, 1156, 180, 115, 421, 561, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761, 4761]\n",
      "文本分类label: 4\n",
      "mask: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "句子长度: 38\n"
     ]
    }
   ],
   "source": [
    "print('文本：',train_contents[1])\n",
    "print('ont hot编码:',train_text[1])\n",
    "print('文本分类label:',train_labels[1])\n",
    "print('mask:',train_masks[1])\n",
    "print('句子长度:',len(train_text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "train_data = Dataset(train_text, train_labels,train_masks)\n",
    "train_dataloader = DataLoader(train_data,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = Dataset(dev_text, dev_labels,dev_masks)\n",
    "dev_dataloader = DataLoader(dev_data,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self,vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        #注意力头数\n",
    "        self.num_attention_heads = 8 \n",
    "        #字编码维度\n",
    "        self.embedding_size = 240\n",
    "        #每个头多大:30\n",
    "        self.attention_head_size = int(self.embedding_size / self.num_attention_heads)\n",
    "        \n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        \n",
    "        self.class_nums = 10\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        \n",
    "        \n",
    "config = Config(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(config.vocab_size,240)\n",
    "        self.config = config\n",
    "        self.query = nn.Linear(config.embedding_size, config.all_head_size)#240 * 240\n",
    "        self.key = nn.Linear(config.embedding_size, config.all_head_size)#240 * 240\n",
    "        self.value = nn.Linear(config.embedding_size, config.all_head_size)#240 * 240\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.fc = nn.Linear(config.embedding_size,config.class_nums)\n",
    "        \n",
    "    def forward(self,text,mask):\n",
    "        #text : batch,38; mask: batch,38\n",
    "        text_embedding = self.embedding(text) # batch,38,240\n",
    "\n",
    "        Q = self.query(text_embedding) # batch,38,240\n",
    "        K = self.key(text_embedding) # batch,38,240\n",
    "        V = self.value(text_embedding) # batch,38,240\n",
    "        \n",
    "        # batch seq_len 240 -> batch 8 seq_len 30\n",
    "        new_shape = Q.size()[:-1] + (self.config.num_attention_heads , self.config.attention_head_size)\n",
    "        Q = Q.view(*new_shape).permute(0,2,1,3) # batch 8 38 30\n",
    "        K = K.view(*new_shape).permute(0,2,1,3) # batch 8 38 30\n",
    "        V = V.view(*new_shape).permute(0,2,1,3) # batch 8 38 30\n",
    "        # (QK^T)/sqrt(d)\n",
    "        attention_scores = torch.matmul(Q,K.transpose(-1,-2))# batch 8 38 30 * batch 8 30 38 = batch 8 38 38\n",
    "        attention_scores = attention_scores / math.sqrt(self.config.attention_head_size)\n",
    "        \n",
    "        \n",
    "        # 变换mask\n",
    "        extended_img_mask = mask.unsqueeze(1).unsqueeze(2) # batch 1 1 38 思考广播机制\n",
    "        extended_img_mask = (1.0 - extended_img_mask) * -10000.0 # batch 1 1 38\n",
    "        attention_scores = attention_scores + extended_img_mask\n",
    "        \n",
    "        #softmax((QK^T)/sqrt(d))\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)# batch 8 38\n",
    "        attention_probs = self.dropout(attention_probs).to(torch.float) \n",
    "        \n",
    "        res = torch.matmul(attention_probs,V)# batch 8 38 30\n",
    "        \n",
    "        res = res.permute(0,2,1,3)#.contiguous() #batch 38 8 30\n",
    "        ori_shape = res.size()[:-2] + (self.config.all_head_size,)\n",
    "        res = res.view(*ori_shape) # batch 38 240\n",
    "        \n",
    "        res = torch.mean(res,axis = 1) # batch 240 \n",
    "        pred = self.fc(res)\n",
    "        \n",
    "        return pred , attention_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 4, 2])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(5,4,3)\n",
    "b = torch.rand(10,1,3,2)\n",
    "torch.matmul(a,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class SelfAttention_Print(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(SelfAttention_Print, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size,240)\n",
    "        self.config = config\n",
    "        self.query = nn.Linear(config.embedding_size, config.all_head_size)#240 * 240\n",
    "        self.key = nn.Linear(config.embedding_size, config.all_head_size)#240 * 240\n",
    "        self.value = nn.Linear(config.embedding_size, config.all_head_size)#240 * 240\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.fc = nn.Linear(config.embedding_size,config.class_nums)\n",
    "        \n",
    "    def forward(self,text,mask):\n",
    "        #text : batch,38; mask: batch,38\n",
    "        text_embedding = self.embedding(text) # batch,38,240\n",
    "        print('text_embedding:',text_embedding.shape)\n",
    "        \n",
    "        Q = self.query(text_embedding) # batch,38,240\n",
    "        K = self.key(text_embedding) # batch,38,240\n",
    "        V = self.value(text_embedding) # batch,38,240\n",
    "        print('Q:',Q.shape)\n",
    "        print('K:',K.shape)\n",
    "        print('V:',V.shape)\n",
    "        \n",
    "        # batch seq_len 240 -> batch 8 seq_len 30\n",
    "        new_shape = Q.size()[:-1] + (self.config.num_attention_heads , self.config.attention_head_size)\n",
    "        Q = Q.view(*new_shape).permute(0,2,1,3) # batch 8 38 30\n",
    "        K = K.view(*new_shape).permute(0,2,1,3) # batch 8 38 30\n",
    "        V = V.view(*new_shape).permute(0,2,1,3) # batch 8 38 30\n",
    "        print('Q:',Q.shape)\n",
    "        print('K:',K.shape)\n",
    "        print('V:',V.shape)\n",
    "        \n",
    "        # (QK^T)/sqrt(d)\n",
    "        attention_scores = torch.matmul(Q,K.transpose(-1,-2))# batch 8 38 30 * batch 8 30 38 = batch 8 38 38\n",
    "        attention_scores = attention_scores / math.sqrt(self.config.attention_head_size)\n",
    "        print('attention_scores:',attention_scores.shape)\n",
    "        \n",
    "        # 变换mask\n",
    "        extended_img_mask = mask.unsqueeze(1).unsqueeze(2) # batch 1 1 38 思考广播机制\n",
    "        extended_img_mask = (1 - extended_img_mask) * -10000 # batch 1 1 38\n",
    "        print('extended_img_mask:',extended_img_mask.dtype)\n",
    "        print('attention_scores:',attention_scores.dtype)\n",
    "        print('extended_img_mask:',extended_img_mask.shape)\n",
    "        attention_scores = attention_scores + extended_img_mask\n",
    "        attention_scores = attention_scores.to(torch.float) \n",
    "        print('attention_scores:',attention_scores.shape)\n",
    "        #softmax((QK^T)/sqrt(d))\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)# batch 8 38 38\n",
    "        print('after softmax attention_scores:',attention_scores.shape)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        res = torch.matmul(attention_probs,V)# batch 8 38 30\n",
    "        print('res:',res.shape)\n",
    "        print(res.is_contiguous()) #True\n",
    "        res = res.permute(0,2,1,3)#.contiguous() #batch 38 8 30\n",
    "        print(res.is_contiguous()) #False\n",
    "        ori_shape = res.size()[:-2] + (self.config.all_head_size,)\n",
    "        res = res.reshape(*ori_shape) # batch 38 240 view必须要一个连续存储的tensor 必须is_contiguous() 是True\n",
    "        print('res:',res.shape)\n",
    "        res = torch.mean(res,axis = 1)#batch 240\n",
    "        print('res:',res.shape)\n",
    "        pred = self.fc(res)\n",
    "        print('pred:',pred.shape)#batch 10\n",
    "        return pred , attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2424868206912\n",
      "tensor([[0.3695, 0.1577, 0.3516],\n",
      "        [0.2821, 0.0894, 0.9786]])\n",
      "tensor([[1.0000, 0.1577, 0.3516],\n",
      "        [0.2821, 0.0894, 0.9786]])\n"
     ]
    }
   ],
   "source": [
    "# view reshape 原变量 : 加不加都会改，相当于都没有深拷贝 一般是a = a.reshape() reshape 是 先continuous 再view两个操作（回去看看） \n",
    "# permute transpose 如果加了.contiguous() 就能深拷贝了\n",
    "import copy\n",
    "a = torch.rand((2,3))\n",
    "b = a.contiguous()\n",
    "\n",
    "print(id(a.data))\n",
    "id(b.data)\n",
    "print(a)\n",
    "b[0][0] = 1\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttention_Print(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_embedding: torch.Size([32, 38, 240])\n",
      "Q: torch.Size([32, 38, 240])\n",
      "K: torch.Size([32, 38, 240])\n",
      "V: torch.Size([32, 38, 240])\n",
      "Q: torch.Size([32, 8, 38, 30])\n",
      "K: torch.Size([32, 8, 38, 30])\n",
      "V: torch.Size([32, 8, 38, 30])\n",
      "attention_scores: torch.Size([32, 8, 38, 38])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 38])\n",
      "attention_scores: torch.Size([32, 8, 38, 38])\n",
      "after softmax attention_scores: torch.Size([32, 8, 38, 38])\n",
      "res: torch.Size([32, 8, 38, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 38, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "model = model.train()\n",
    "for i,batch in enumerate(train_dataloader):\n",
    "    if i == 0:\n",
    "        train_text,train_labels,train_masks = batch\n",
    "        pred,attention_probs = model(train_text,train_masks)\n",
    "        loss = criterion(pred,train_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5625/5625 [05:08<00:00, 18.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5625/5625 [06:02<00:00, 15.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5625/5625 [06:10<00:00, 15.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5625/5625 [06:02<00:00, 15.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5625/5625 [05:55<00:00, 15.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model = SelfAttention(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.train()\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    for i,batch in enumerate(tqdm(train_dataloader)):\n",
    "        train_text,train_labels,train_masks = batch\n",
    "        pred,attention_probs = model(train_text,train_masks)\n",
    "        loss = criterion(pred,train_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 3/313 [00:00<00:11, 26.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                              | 10/313 [00:00<00:10, 27.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▌                                                                             | 14/313 [00:00<00:10, 29.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▍                                                                           | 21/313 [00:00<00:09, 30.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▉                                                                          | 27/313 [00:00<00:09, 29.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▊                                                                         | 30/313 [00:01<00:10, 26.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▎                                                                       | 36/313 [00:01<00:10, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▎                                                                      | 40/313 [00:01<00:09, 27.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▍                                                                    | 48/313 [00:01<00:08, 29.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▍                                                                   | 52/313 [00:01<00:09, 27.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████                                                                  | 58/313 [00:02<00:09, 26.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████                                                                 | 62/313 [00:02<00:08, 28.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████▊                                                                | 65/313 [00:02<00:09, 26.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▌                                                               | 68/313 [00:02<00:11, 21.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▏                                                             | 74/313 [00:02<00:10, 23.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▉                                                            | 81/313 [00:02<00:08, 26.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|█████████████████████▋                                                           | 84/313 [00:03<00:08, 26.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████▌                                                         | 91/313 [00:03<00:08, 26.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▌                                                        | 95/313 [00:03<00:07, 28.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████                                                      | 102/313 [00:03<00:07, 26.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████████████████▊                                                     | 105/313 [00:03<00:07, 26.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████▉                                                   | 113/313 [00:04<00:06, 29.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|█████████████████████████████▉                                                  | 117/313 [00:04<00:06, 30.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▉                                                | 125/313 [00:04<00:06, 31.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████████████████████▉                                               | 129/313 [00:04<00:05, 30.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████████████████████▉                                              | 133/313 [00:04<00:05, 30.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████                                            | 141/313 [00:05<00:06, 28.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████████████████████████████████████▊                                           | 144/313 [00:05<00:05, 28.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████▌                                         | 151/313 [00:05<00:05, 28.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████▌                                        | 155/313 [00:05<00:05, 30.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████▋                                      | 163/313 [00:05<00:04, 30.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████████████████████████▋                                     | 167/313 [00:05<00:05, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|███████████████████████████████████████████▍                                    | 170/313 [00:06<00:05, 25.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████▉                                   | 176/313 [00:06<00:05, 25.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|██████████████████████████████████████████████                                  | 180/313 [00:06<00:04, 26.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████                                | 188/313 [00:06<00:04, 30.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████                              | 196/313 [00:06<00:03, 32.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|███████████████████████████████████████████████████                             | 200/313 [00:07<00:03, 33.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████▏                          | 208/313 [00:07<00:03, 33.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████▏                         | 212/313 [00:07<00:03, 33.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████████████████████████████████▏                        | 216/313 [00:07<00:03, 31.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████▉                       | 223/313 [00:07<00:03, 26.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████████████████████████████▊                      | 226/313 [00:07<00:03, 26.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████████████████████████████████▌                    | 233/313 [00:08<00:02, 27.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████▌                  | 241/313 [00:08<00:02, 30.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████▌                 | 245/313 [00:08<00:02, 30.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████▋               | 253/313 [00:08<00:01, 30.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|█████████████████████████████████████████████████████████████████▋              | 257/313 [00:08<00:02, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|██████████████████████████████████████████████████████████████████▍             | 260/313 [00:09<00:02, 24.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████▉            | 266/313 [00:09<00:01, 25.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|█████████████████████████████████████████████████████████████████████           | 270/313 [00:09<00:01, 27.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████▌         | 276/313 [00:09<00:01, 25.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████▎        | 279/313 [00:09<00:01, 26.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████████▎      | 287/313 [00:10<00:00, 29.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|██████████████████████████████████████████████████████████████████████████▍     | 291/313 [00:10<00:00, 29.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████▍   | 299/313 [00:10<00:00, 30.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████████████████████████████████████████████████████████████████████████▍  | 303/313 [00:10<00:00, 29.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|██████████████████████████████████████████████████████████████████████████████▏ | 306/313 [00:10<00:00, 25.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 312/313 [00:11<00:00, 23.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 33, 240])\n",
      "K: torch.Size([32, 33, 240])\n",
      "V: torch.Size([32, 33, 240])\n",
      "Q: torch.Size([32, 8, 33, 30])\n",
      "K: torch.Size([32, 8, 33, 30])\n",
      "V: torch.Size([32, 8, 33, 30])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([32, 1, 1, 33])\n",
      "attention_scores: torch.Size([32, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([32, 8, 33, 33])\n",
      "res: torch.Size([32, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([32, 33, 240])\n",
      "res: torch.Size([32, 240])\n",
      "pred: torch.Size([32, 10])\n",
      "text_embedding: torch.Size([16, 33, 240])\n",
      "Q: torch.Size([16, 33, 240])\n",
      "K: torch.Size([16, 33, 240])\n",
      "V: torch.Size([16, 33, 240])\n",
      "Q: torch.Size([16, 8, 33, 30])\n",
      "K: torch.Size([16, 8, 33, 30])\n",
      "V: torch.Size([16, 8, 33, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores: torch.Size([16, 8, 33, 33])\n",
      "extended_img_mask: torch.float64\n",
      "attention_scores: torch.float32\n",
      "extended_img_mask: torch.Size([16, 1, 1, 33])\n",
      "attention_scores: torch.Size([16, 8, 33, 33])\n",
      "after softmax attention_scores: torch.Size([16, 8, 33, 33])\n",
      "res: torch.Size([16, 8, 33, 30])\n",
      "True\n",
      "False\n",
      "res: torch.Size([16, 33, 240])\n",
      "res: torch.Size([16, 240])\n",
      "pred: torch.Size([16, 10])\n",
      "Accuracy: 0.1193\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "model = model.eval()\n",
    "total_pred = np.array([],dtype = np.int)\n",
    "total_true = np.array([],dtype = np.int)\n",
    "all_attn_probs = []\n",
    "for i,batch in enumerate(tqdm(dev_dataloader)):\n",
    "    dev_text,dev_labels,dev_masks = batch\n",
    "    pred,attention_probs = model(dev_text,dev_masks)\n",
    "    pred_label = torch.argmax(pred,axis = -1)\n",
    "    total_pred = np.append(total_pred,pred_label)\n",
    "    total_true = np.append(total_true,dev_labels)\n",
    "    all_attn_probs.append(attention_probs)\n",
    "print('Accuracy:',metrics.accuracy_score(total_pred,total_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 312/312 [00:19<00:00, 16.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 8, 33, 33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_attn_probs = all_attn_probs[0]\n",
    "for i in trange(1,len(all_attn_probs)):\n",
    "    res_attn_probs = torch.vstack((res_attn_probs,all_attn_probs[i]))\n",
    "print(res_attn_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 33, 33])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_attn_probs = torch.mean(res_attn_probs,axis = 1)\n",
    "avg_attn_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['axes.unicode_minus'] = False   # 步骤二（解决坐标轴负数的负号显示问题）\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "#设置标注前后左右的距离\n",
    "scores = np.random.rand(3,2)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "        )\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "# plt.xticks(np.arange(3), [1,2,3], rotation=45)\n",
    "plt.yticks(np.arange(3), ['我','爱',4])\n",
    "\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(r'C:\\Users\\chunhui\\Desktop\\南京大学\\NLP组\\TextCNN_Mindspore\\THUCNews\\checkpoint\\vocab.pkl','rb') as f:\n",
    "    vocab_dict = pkl.load(f)\n",
    "id2word_dict = {value:key for key,value in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_choose_one_visualize(id2word_dict,text,label,avg_attn_score,idx=None):\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(text))\n",
    "    text_one = text[idx].tolist()\n",
    "    label_one = label[idx]\n",
    "    ori_text_list = [id2word_dict[num] for num in text_one]\n",
    "    ori_test = ''.join(ori_text_list)\n",
    "    print('原文本:',ori_test)\n",
    "    print('label:',label_one)\n",
    "    attn_score = avg_attn_score[idx].tolist()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attn_score, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(text_one)), ori_text_list)\n",
    "    plt.yticks(np.arange(len(text_one)), ori_text_list)\n",
    "\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文本: 假军官交友网站骗两女子10余万元<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "label: tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_choose_one_visualize(id2word_dict,dev_text,dev_labels,avg_attn_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
